# -*- coding: utf-8 -*-
"""NHS_Content_Extraction_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1NceHSVNnBdct3R5MATa2i4--oDOuxw
"""

!pip install beautifulsoup4

documents_directory = '/nhs_source'
input_folder = documents_directory
output_folder = "/nhs_content"

# file name collection - working. All content will be stored on the source folder.

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os

# Specify the URL of the main web page
url = "https://www.nhs.uk/conditions/"

# Send a GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find all the links on the page
    links = soup.find_all('a')

    # Create the documents directory if it doesn't exist
#    documents_directory = '/content/drive/MyDrive/data/nhs_source_medicine'
    os.makedirs(documents_directory, exist_ok=True)

    # Extract and store the content from each sub-URL
    for link in links:
        sub_url = urljoin(url, link.get('href'))
        sub_response = requests.get(sub_url)

        if sub_response.status_code == 200:
            sub_soup = BeautifulSoup(sub_response.content, 'html.parser')
            sub_content = sub_soup.find('div', class_='nhsuk-grid-row').get_text()

            # Get the file name from the sub-URL
            file_name = os.path.join(documents_directory, sub_url.split("/")[-2] + ".txt")
            with open(file_name, 'w', encoding='utf-8') as file:
                file.write(sub_content)
                print(f"Content extracted and stored in '{file_name}' file.")
        else:
            print(f"Failed to retrieve the sub-URL: {sub_url}")
else:
    print("Failed to retrieve the main web page.")

# Content extraction and store on the online folder for future indexing purpose
import os
import requests
from bs4 import BeautifulSoup

def process_files(input_folder, output_folder):
    # Get all file names in the input folder
    file_names = os.listdir(input_folder)

    # Process each file
    for file_name in file_names:
        # Form the URL
        condition_name = os.path.splitext(file_name)[0]
        url = f"https://www.nhs.uk/conditions/{condition_name}/"
        print(f"Extracting content from URL: {url}")

        extracted_text = extract_content(url)

        # Create an output file path
        output_file_path = os.path.join(output_folder, f"{condition_name}.txt")

        # Save the extracted content to the output file
        with open(output_file_path, "w") as output_file:
            output_file.write(extracted_text)

        print(f"Content saved to: {output_file_path}")


def extract_content(url):
    # Send a GET request to the URL and get the HTML content
    response = requests.get(url)
    html_content = response.content

    # Create a BeautifulSoup object to parse the HTML
    soup = BeautifulSoup(html_content, "html.parser")

    # Find all the <section> tags that contain information
    sections = soup.find_all("section")

    # Extract the text from each <section> tag and return it
    extracted_text = ""
    for section in sections:
        section_text = section.text.strip()
        extracted_text += section_text + "\n"

    return extracted_text


# Provide the input and output folder paths
#input_folder = "/nhs_source"
#output_folder = "/nhs_content"

# Call the function to process the files
process_files(input_folder, output_folder)