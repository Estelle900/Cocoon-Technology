# -*- coding: utf-8 -*-
"""Ultrasound_EDU_Embedding_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tCoEyGBsH38GZgL-BLAiNOaDCAZX4gHi
"""

path_content_source = "/nhs_content_2"
path_vector_db = "/nhs_vector_db4"

!pip install langchain
!pip install llama-index

import os
import json
import openai
import sys

from llama_index import VectorStoreIndex, SimpleDirectoryReader, LangchainEmbedding, PromptHelper, LLMPredictor, ServiceContext, set_global_service_context
from llama_index import load_index_from_storage, StorageContext, QuestionAnswerPrompt
from llama_index.llms import OpenAI
from llama_index.indices.postprocessor.node import SimilarityPostprocessor
from IPython.display import Markdown, display

os.environ['OPENAI_API_KEY'] = "NA"

# Set up GPT3.5-Turbo
service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0))
set_global_service_context(service_context)
openai.api_key = os.environ["OPENAI_API_KEY"]



# Prompt will be used to generate rich response,
# Modify this prompt to add instruction

QA_PROMPT_TMPL = (
    "Ultrasound Physician Assistant to based on query and proivde correct training cause. "
    "We have provided context information below. \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "Given this information, here is question related to untrasound usage: {query_str}\n"
    "Please provide revelant training cause from the context and source URL"
    )
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

documents = SimpleDirectoryReader(path_content_source).load_data()

# Building the index file
index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context,
    show_progress=True
)

# save index to disk
index.set_index_id("nhs")
index.storage_context.persist(path_vector_db)

# rebuild storage context
storage_context = StorageContext.from_defaults(persist_dir=path_vector_db)
# load index. Ensure service_context add here to use customized LLM
index = load_index_from_storage(storage_context=storage_context,service_context=service_context)

# add in prompt template and synthesizer...
query_engine = index.as_query_engine(
    service_context=service_context,
    text_qa_template=QA_PROMPT,
    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.2)],
    verbose=True,
)

while True:
    query_text = input("Enter your question (or 'exit' to quit): ")

    if query_text.lower() == "exit":
        print("Exiting the program...")
        break

    response = query_engine.query(query_text)
    response_ext = Markdown(f"{response.response}").data

    if response_ext == "None":
        response_ext="We were unable to find the answer in our current knowledge database. Kindly ask another question."

    print(response_ext)