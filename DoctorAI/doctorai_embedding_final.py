# -*- coding: utf-8 -*-
"""DoctorAI_Embedding_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YYJW09sOk1CSv7drLBk6UBHyWLcPj1Ti
"""

path_content_source = "/nhs_content"
path_vector_db = "/nhs_vector_db"

!pip install langchain
!pip install llama-index

import os
import json
import openai
import sys

from llama_index import VectorStoreIndex, SimpleDirectoryReader, LangchainEmbedding, PromptHelper, LLMPredictor, ServiceContext, set_global_service_context
from llama_index import load_index_from_storage, StorageContext, QuestionAnswerPrompt
from llama_index.llms import OpenAI
from llama_index.indices.postprocessor.node import SimilarityPostprocessor
from IPython.display import Markdown, display

from llama_index import Document

# Create a document with filename in metadata

document = Document(
    text='text',
    metadata={
        'filename': '<doc_file_name>',
        'category': '<category>'
    }
)

document.metadata = {'filename': '<doc_file_name>'}

filename_fn = lambda filename: {'file_name': filename}

os.environ['OPENAI_API_KEY'] = "NA"

# Set up GPT3.5-Turbo
service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0))
set_global_service_context(service_context)
openai.api_key = os.environ["OPENAI_API_KEY"]

# Prompt will be used to generate rich response,
# Modify this prompt to add instruction

QA_PROMPT_TMPL = (
    "We have provided context information below. \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "Given this information, Explain and add conclusion at the end: {query_str}\n"
    )
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

documents = SimpleDirectoryReader(path_content_source, file_metadata=filename_fn).load_data()

# Building the index file
index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context,
    show_progress=True
)

# save index to disk
index.set_index_id("nhs")
index.storage_context.persist(path_vector_db)

# rebuild storage context
storage_context = StorageContext.from_defaults(persist_dir=path_vector_db)
# load index. Ensure service_context add here to use customized LLM
index = load_index_from_storage(storage_context=storage_context,service_context=service_context)

# add in prompt template and synthesizer...
query_engine = index.as_query_engine(
    service_context=service_context,
    text_qa_template=QA_PROMPT,
    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.2)],
    verbose=True,
)

while True:
    query_text = input("Enter your question (or 'exit' to quit): ")

    if query_text.lower() == "exit":
        print("Exiting the program...")
        break

    response = query_engine.query(query_text)
    response_ext = Markdown(f"{response.response}").data

    response_ext = response_ext + "\n\nReference used to form this response:\n"

    # Assuming 'response' is the response from your query
    for source_node in response.source_nodes:
        filename = source_node.node.metadata.get('file_name',None)
        filename = filename.split('/')[-1].replace('.txt', '')
        url = "https://www.nhs.uk/conditions/" + filename + "/"
        response_ext = response_ext + url + "\n"


    if response_ext == "None":
        response_ext="We were unable to find the answer in our current knowledge database. Kindly ask another question."

    print(response_ext)